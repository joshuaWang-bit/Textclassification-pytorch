{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "unlikely-export",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "import pandas as pd\n",
    "import torchtext\n",
    "import jieba \n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fifth-focus",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_address = '../Data_Cn/'\n",
    "\n",
    "stopwords_path = '../utils/stopwords/cn_stopwords.txt'\n",
    "stopwords = open(stopwords_path).read().split('\\n')\n",
    "\n",
    "def cut(sentence):\n",
    "    return [token for token in jieba.lcut(sentence) if token not in stopwords]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "objective-catch",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#声明一个Field对象，对象里面填的就是需要对文本进行哪些操作，比如这里lower=True英文大写转小写,tokenize=cut对于文本分词采用之前定义好的cut函数，sequence=True表示输入的是一个sequence类型的数据，还有其他更多操作可以参考文档\n",
    "TEXT = torchtext.legacy.data.Field(sequential=True,lower=True,tokenize=cut)\n",
    "#声明一个标签的LabelField对象，sequential=False表示标签不是sequence，dtype=torch.int64标签转化成整形\n",
    "LABEL = torchtext.legacy.data.LabelField(sequential=False, dtype=torch.int64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "danish-arizona",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /tmp/jieba.cache\n",
      "Loading model cost 1.718 seconds.\n",
      "Prefix dict has been built successfully.\n"
     ]
    }
   ],
   "source": [
    "train_dataset,dev_dataset,test_dataset = torchtext.legacy.data.TabularDataset.splits(\n",
    "      path=data_address,  #文件存放路径\n",
    "      format='tsv',   #文件格式\n",
    "      skip_header=False,  #是否跳过表头，我这里数据集中没有表头，所以不跳过\n",
    "      train='train_data.tsv',  \n",
    "      validation='test_data.tsv',\n",
    "      test='dev_data.tsv',    \n",
    "      fields=[('label',LABEL),('content',TEXT)] # 定义数据对应的表头\n",
    "  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "first-title",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'label': '0', 'content': ['马晓旭', '意外', '受伤', '国奥', '警惕', ' ', '无奈', '大雨', '格外', '青睐', '殷家', '军', '记者', '傅亚雨', '沈阳', '报道', ' ', '来到', '沈阳', '国奥队', '依然', '没有', '摆脱', '雨水', '困扰', '月', '31', '日', '下午', '点', '国奥队', '日常', '训练', '再度', '大雨', '干扰', '无奈', '之下', '队员', '慢跑', '25', '分钟', '草草收场', '31', '日', '上午', '10', '点', '国奥队', '奥体中心', '外场', '训练', '天', '阴沉沉', '气象预报', '显示', '当天', '下午', '沈阳', '大雨', '幸好', '队伍', '上午', '训练', '没有', '干扰', '下午', '点', '球队', '抵达', '训练场', '时', '大雨', '已经', '几个', '小时', '丝毫', '没有', '停下来', '意思', '抱', '试一试', '态度', '球队', '当天', '下午', '例行', '训练', '25', '分钟', '过去', '天气', '没有', '转好', '迹象', '保护', '球员', '国奥队', '决定', '中止', '当天', '训练', '全队', '立即', '返回', '酒店', '雨', '中', '训练', '足球队', '稀罕', '事', '奥运会', '即将', '之前', '全队', '变得', '娇贵', '沈阳', '最后', '一周', '训练', '国奥队', '保证', '现有', '球员', '不再', '出现意外', '伤病', '情况', '影响', '正式', '比赛', '这一', '阶段', '控制', '训练', '受伤', '控制', '感冒', '疾病', '出现', '队伍', '放在', '相当', '重要', '位置', '抵达', '沈阳', '之后', '中', '后卫', '冯萧霆', '一直', '没有', '训练', '冯萧霆', '月', '27', '日', '长春', '患上', '感冒', '没有', '参加', '29', '日', '塞尔维亚', '热身赛', '队伍', '介绍', '说', '冯萧霆', '没有', '出现', '发烧', '症状', '安全', '见', '两天', '静养', '休息', '感冒', '彻底', '之后', '恢复', '训练', '冯萧霆', '例子', '国奥队', '对雨中', '训练', '显得', '特别', '谨慎', '主要', '担心', '球员', '受凉', '引发', '感冒', '造成', '非战斗', '减员', '女足', '队员', '马晓旭', '热身赛', '中', '受伤', '导致', '无缘', '奥运', '前科', '沈阳', '国奥队', '现在', '格外', '警惕', '训练', '中', '不断', '嘱咐', '队员', '注意', '动作', '不能', '再出', '事情', '一位', '工作人员', '表示', '长春', '沈阳', '雨水', '一路', '伴随', '国奥队', '邪', '走', '雨', '长春', '几次', '训练', '大雨', '搅和', '没想到', '沈阳', '碰到', '这种', '事情', '一位', '国奥', '球员', '雨水', '青睐', '不解']}\n"
     ]
    }
   ],
   "source": [
    "print(vars(train_dataset.examples[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "chronic-stanford",
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained_name = 'sgns.sogounews.bigram-char' # 预训练词向量文件名\n",
    "pretrained_path = '../utils/TEXTCNN_MODEL_CN' #预训练词向量存放路径\n",
    "vectors = torchtext.vocab.Vectors(name=pretrained_name, cache=pretrained_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "shaped-thriller",
   "metadata": {},
   "outputs": [],
   "source": [
    "TEXT.build_vocab(train_dataset, dev_dataset,test_dataset,\n",
    "                 vectors=vectors)\n",
    "LABEL.build_vocab(train_dataset, dev_dataset,test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "therapeutic-graph",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_iter, dev_iter,test_iter = torchtext.legacy.data.BucketIterator.splits(\n",
    "        (train_dataset, dev_dataset,test_dataset), #需要生成迭代器的数据集\n",
    "        batch_sizes=(128, 128,128), # 每个迭代器分别以多少样本为一个batch\n",
    "        sort_key=lambda x: len(x.content) #按什么顺序来排列batch，这里是以句子的长度，就是上面说的把句子长度相近的放在同一个batch里面\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "abandoned-recycling",
   "metadata": {},
   "outputs": [],
   "source": [
    "#模型\n",
    "class TextCNN(nn.Module):\n",
    "    def __init__(self, \n",
    "                 class_num, # 最后输出的种类数 \n",
    "                 filter_sizes, # 卷积核的长也就是滑动窗口的长 \n",
    "                 filter_num,   # 卷积核的数量 \n",
    "                 vocabulary_size, # 词表的大小\n",
    "                 embedding_dimension, # 词向量的维度\n",
    "                 vectors, # 词向量\n",
    "                 dropout): # dropout率\n",
    "        super(TextCNN, self).__init__() # 继承nn.Module\n",
    "\n",
    "        chanel_num = 1  # 通道数，也就是一篇文章一个样本只相当于一个feature map\n",
    "\n",
    "        self.embedding = nn.Embedding(vocabulary_size, embedding_dimension) # 嵌入层 \n",
    "        self.embedding = self.embedding.from_pretrained(vectors) #嵌入层加载预训练词向量\n",
    "\n",
    "        self.convs = nn.ModuleList(\n",
    "            [nn.Conv2d(chanel_num, filter_num, (fsz, embedding_dimension)) for fsz in filter_sizes])  # 卷积层\n",
    "        self.dropout = nn.Dropout(dropout) # dropout\n",
    "        self.fc = nn.Linear(len(filter_sizes) * filter_num, class_num) #全连接层\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x维度[句子长度,一个batch中所包含的样本数] 例:[3451,128]\n",
    "        x = self.embedding(x) # #经过嵌入层之后x的维度，[句子长度,一个batch中所包含的样本数,词向量维度] 例：[3451,128,300]\n",
    "        x = x.permute(1,0,2) # permute函数将样本数和句子长度换一下位置，[一个batch中所包含的样本数,句子长度,词向量维度] 例：[128,3451,300]\n",
    "        x = x.unsqueeze(1) # # conv2d需要输入的是一个四维数据，所以新增一维feature map数 unsqueeze(1)表示在第一维处新增一维，[一个batch中所包含的样本数,一个样本中的feature map数，句子长度,词向量维度] 例：[128,1,3451,300]\n",
    "        x = [conv(x) for conv in self.convs] # 与卷积核进行卷积，输出是[一个batch中所包含的样本数,卷积核数，句子长度-卷积核size+1,1]维数据,因为有[3,4,5]三张size类型的卷积核所以用列表表达式 例：[[128,16,3459,1],[128,16,3458,1],[128,16,3457,1]]\n",
    "        x = [sub_x.squeeze(3) for sub_x in x]#squeeze(3)判断第三维是否是1，如果是则压缩，如不是则保持原样 例：[[128,16,3459],[128,16,3458],[128,16,3457]]\n",
    "        x = [F.relu(sub_x) for sub_x in x] # ReLU激活函数激活，不改变x维度 \n",
    "        x = [F.max_pool1d(sub_x,sub_x.size(2)) for sub_x in x] # 池化层，根据之前说的原理，max_pool1d要取出每一个滑动窗口生成的矩阵的最大值，因此在第二维上取最大值 例：[[128,16,1],[128,16,1],[128,16,1]]\n",
    "        x = [sub_x.squeeze(2) for sub_x in x] # 判断第二维是否为1，若是则压缩 例：[[128,16],[128,16],[128,16]]\n",
    "        x = torch.cat(x, 1) # 进行拼接，例：[128,48]\n",
    "        x = self.dropout(x) # 去除掉一些神经元防止过拟合，注意dropout之后x的维度依旧是[128,48]，并不是说我dropout的概率是0.5，去除了一半的神经元维度就变成了[128,24]，而是把x中的一些神经元的数据根据概率全部变成了0，维度依旧是[128,48]\n",
    "        logits = self.fc(x) # 全接连层 例：输入x是[128,48] 输出logits是[128,10]\n",
    "        return logits\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "color-catch",
   "metadata": {},
   "outputs": [],
   "source": [
    "#初始化设置\n",
    "class_num = len(LABEL.vocab) # 类别数目\n",
    "filter_size = [3,4,5]  # 卷积核种类数 \n",
    "filter_num=16   # 卷积核数量\n",
    "vocab_size = len(TEXT.vocab) # 词表大小\n",
    "embedding_dim = TEXT.vocab.vectors.size()[-1] # 词向量维度\n",
    "vectors = TEXT.vocab.vectors # 词向量\n",
    "dropout=0.5 \n",
    "learning_rate = 0.001  # 学习率\n",
    "epochs = 5   # 迭代次数\n",
    "save_dir = './model_textcnn_cn/' # 模型保存路径\n",
    "steps_show = 10   # 每10步查看一次训练集loss和mini batch里的准确率\n",
    "steps_eval = 100  # 每100步测试一下验证集的准确率\n",
    "early_stopping = 1000  # 若发现当前验证集的准确率在1000步训练之后不再提高 一直小于best_acc,则提前停止训练\n",
    "\n",
    "textcnn_model = TextCNN(class_num=class_num,\n",
    "        filter_sizes=filter_size,\n",
    "        filter_num=filter_num,\n",
    "        vocabulary_size=vocab_size,\n",
    "        embedding_dimension=embedding_dim,\n",
    "        vectors=vectors,\n",
    "        dropout=dropout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "elegant-smoke",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义模型保存函数\n",
    "def save(model, save_dir, steps):\n",
    "    if not os.path.isdir(save_dir):\n",
    "        os.makedirs(save_dir)\n",
    "    save_path = 'bestmodel_steps{}.pt'.format(steps)\n",
    "    save_bestmodel_path = os.path.join(save_dir, save_path)\n",
    "    torch.save(model.state_dict(), save_bestmodel_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "finished-belief",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(train_iter, dev_iter, model):\n",
    "    if torch.cuda.is_available(): # 判断是否有GPU，如果有把模型放在GPU上训练，速度质的飞跃\n",
    "        model.cuda()\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate) # 梯度下降优化器，采用Adam\n",
    "    steps = 0\n",
    "    best_acc = 0\n",
    "    last_step = 0\n",
    "    model.train()\n",
    "    cnn_train_acc, cnn_test_acc = [], []\n",
    "    for epoch in range(1, epochs + 1): \n",
    "        for batch in train_iter:\n",
    "            feature, target = batch.content, batch.label\n",
    "            if torch.cuda.is_available(): # 如果有GPU将特征更新放在GPU上\n",
    "                feature,target = feature.cuda(),target.cuda() \n",
    "            optimizer.zero_grad() # 将梯度初始化为0，每个batch都是独立训练地，因为每训练一个batch都需要将梯度归零\n",
    "            logits = model(feature)\n",
    "            loss = F.cross_entropy(logits, target) # 计算损失函数 采用交叉熵损失函数\n",
    "            loss.backward()  # 反向传播\n",
    "            optimizer.step() # 放在loss.backward()后进行参数的更新\n",
    "            steps += 1 \n",
    "            if steps % steps_show == 0: # 每训练多少步计算一次准确率，我这边是1，可以自己修改\n",
    "                corrects = (torch.max(logits, 1)[1].view(target.size()).data == target.data).sum() # logits是[128,10],torch.max(logits, 1)也就是选出第一维中概率最大的值，输出为[128,1],torch.max(logits, 1)[1]相当于把每一个样本的预测输出取出来，然后通过view(target.size())平铺成和target一样的size (128,),然后把与target中相同的求和，统计预测正确的数量\n",
    "                train_acc = 100.0 * corrects / batch.batch_size # 计算每个mini batch中的准确率\n",
    "                #print('epoch={},损失为= {:.6f}  训练准确率={:.4f}'.format(\n",
    "                  #epoch,\n",
    "                  #loss.item(),\n",
    "                  #train_acc))\n",
    "                \n",
    "            if steps % steps_eval == 0: # 每训练100步进行一次验证\n",
    "                dev_acc = dev_eval(dev_iter,model)\n",
    "                if dev_acc > best_acc:\n",
    "                    best_acc = dev_acc\n",
    "                    last_step = steps\n",
    "                    print('保存最好的模型, acc: {:.4f}%\\n'.format(best_acc))\n",
    "                    save(model,save_dir, steps)\n",
    "                else:\n",
    "                    #todo 早停策略\n",
    "                    continue\n",
    "        print('epoch={},训练准确率={}'.format(epoch, train_acc))\n",
    "        print(\"epoch={},测试准确率={}\".format(epoch, best_acc))\n",
    "        cnn_train_acc.append(train_acc)\n",
    "        cnn_test_acc.append(best_acc)\n",
    "    return cnn_train_acc, cnn_test_acc\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "superior-column",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dev_eval(dev_iter,model):\n",
    "    model.eval()\n",
    "    corrects, avg_loss = 0, 0\n",
    "    for batch in dev_iter:\n",
    "        feature, target = batch.content, batch.label\n",
    "        if torch.cuda.is_available():\n",
    "            feature, target = feature.cuda(), target.cuda()\n",
    "        logits = model(feature)\n",
    "        loss = F.cross_entropy(logits, target)\n",
    "        avg_loss += loss.item()\n",
    "        corrects += (torch.max(logits, 1)\n",
    "                    [1].view(target.size()).data == target.data).sum()\n",
    "    size = len(dev_iter.dataset)\n",
    "    avg_loss /= size\n",
    "    accuracy = 100.0 * corrects / size\n",
    "    print('\\nEvaluation - loss: {:.6f}  acc: {:.4f}%({}/{}) \\n'.format(avg_loss,\n",
    "                                                                      accuracy,\n",
    "                                                                      corrects,\n",
    "                                                                      size))\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "reserved-secretariat",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluation - loss: 0.004368  acc: 88.6500%(8865/10000) \n",
      "\n",
      "保存最好的模型, acc: 88.6500%\n",
      "\n",
      "\n",
      "Evaluation - loss: 0.002046  acc: 92.9300%(9293/10000) \n",
      "\n",
      "保存最好的模型, acc: 92.9300%\n",
      "\n",
      "\n",
      "Evaluation - loss: 0.001593  acc: 94.7700%(9477/10000) \n",
      "\n",
      "保存最好的模型, acc: 94.7700%\n",
      "\n",
      "epoch=1,训练准确率=93.75\n",
      "epoch=1,测试准确率=94.7699966430664\n",
      "\n",
      "Evaluation - loss: 0.001302  acc: 95.6500%(9565/10000) \n",
      "\n",
      "保存最好的模型, acc: 95.6500%\n",
      "\n",
      "\n",
      "Evaluation - loss: 0.001072  acc: 96.2800%(9628/10000) \n",
      "\n",
      "保存最好的模型, acc: 96.2800%\n",
      "\n",
      "\n",
      "Evaluation - loss: 0.001038  acc: 96.4600%(9646/10000) \n",
      "\n",
      "保存最好的模型, acc: 96.4600%\n",
      "\n",
      "\n",
      "Evaluation - loss: 0.000967  acc: 96.6500%(9665/10000) \n",
      "\n",
      "保存最好的模型, acc: 96.6500%\n",
      "\n",
      "epoch=2,训练准确率=97.65625\n",
      "epoch=2,测试准确率=96.64999389648438\n",
      "\n",
      "Evaluation - loss: 0.000956  acc: 96.6200%(9662/10000) \n",
      "\n",
      "\n",
      "Evaluation - loss: 0.000929  acc: 96.6600%(9666/10000) \n",
      "\n",
      "保存最好的模型, acc: 96.6600%\n",
      "\n",
      "\n",
      "Evaluation - loss: 0.000904  acc: 96.6100%(9661/10000) \n",
      "\n",
      "\n",
      "Evaluation - loss: 0.000935  acc: 96.3600%(9636/10000) \n",
      "\n",
      "epoch=3,训练准确率=99.21875\n",
      "epoch=3,测试准确率=96.65999603271484\n",
      "\n",
      "Evaluation - loss: 0.000957  acc: 96.5700%(9657/10000) \n",
      "\n",
      "\n",
      "Evaluation - loss: 0.000855  acc: 96.9200%(9692/10000) \n",
      "\n",
      "保存最好的模型, acc: 96.9200%\n",
      "\n",
      "\n",
      "Evaluation - loss: 0.000780  acc: 97.2600%(9726/10000) \n",
      "\n",
      "保存最好的模型, acc: 97.2600%\n",
      "\n",
      "\n",
      "Evaluation - loss: 0.000727  acc: 97.3600%(9736/10000) \n",
      "\n",
      "保存最好的模型, acc: 97.3600%\n",
      "\n",
      "epoch=4,训练准确率=99.21875\n",
      "epoch=4,测试准确率=97.36000061035156\n",
      "\n",
      "Evaluation - loss: 0.000763  acc: 97.3200%(9732/10000) \n",
      "\n",
      "\n",
      "Evaluation - loss: 0.000918  acc: 96.6300%(9663/10000) \n",
      "\n",
      "\n",
      "Evaluation - loss: 0.000801  acc: 97.0100%(9701/10000) \n",
      "\n",
      "\n",
      "Evaluation - loss: 0.000810  acc: 97.1300%(9713/10000) \n",
      "\n",
      "epoch=5,训练准确率=99.21875\n",
      "epoch=5,测试准确率=97.36000061035156\n"
     ]
    }
   ],
   "source": [
    "cnn_train_acc, cnn_test_acc=train(train_iter,dev_iter,textcnn_model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "respiratory-nurse",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluation - loss: 0.001395  acc: 94.5000%(4725/5000) \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(94.5000, device='cuda:0')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dev_eval(test_iter,textcnn_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "spare-bumper",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'plt' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-e1a44b78b68d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcnn_train_acc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcnn_test_acc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mylim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mymin\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mymax\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1.01\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"train\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"test\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'plt' is not defined"
     ]
    }
   ],
   "source": [
    "plt.plot(cnn_train_acc)\n",
    "plt.plot(cnn_test_acc)\n",
    "plt.ylim(ymin=0.5, ymax=1.01)\n",
    "plt.legend([\"train\", \"test\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
