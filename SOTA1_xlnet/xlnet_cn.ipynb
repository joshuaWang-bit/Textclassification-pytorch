{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aquatic-arthritis",
   "metadata": {},
   "source": [
    "# Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "clinical-tenant",
   "metadata": {},
   "outputs": [],
   "source": [
    "#中文采用了清华NLP组提供的THUCNews新闻文本分类数据集的子集\n",
    "destination_folder = './model_xlnet_cn'\n",
    "data_address = '../Data_Cn/'\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"3\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "found-martin",
   "metadata": {},
   "source": [
    "# Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ongoing-liberal",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchtext\n",
    "from torchtext.legacy import data\n",
    "from torchtext.legacy import datasets\n",
    "import torchtext\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "from torchtext.legacy.data import Field, TabularDataset, BucketIterator, Iterator\n",
    "import torch.nn as nn\n",
    "from transformers import XLNetModel, XLNetTokenizer, XLNetForSequenceClassification\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "import spacy\n",
    "from tqdm import tqdm, trange\n",
    "import random\n",
    "import math\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from transformers import AdamW\n",
    "use_cuda=torch.cuda.is_available()\n",
    "device=torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "SEED=1234\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "if use_cuda:\n",
    "    torch.cuda.manual_seed(SEED)\n",
    "\n",
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "rubber-handling",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "train_data = pd.read_csv(data_address+'cnews.train.txt',sep='\\t',names=['label','content'])\n",
    "test_data = pd.read_csv(data_address+'cnews.test.txt',sep='\\t',names=['label','content'])\n",
    "dev_data = pd.read_csv(data_address+'cnews.val.txt',sep='\\t',names=['label','content'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "graphic-hierarchy",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all = pd.concat([train_data,test_data,dev_data])\n",
    "df_all = df_all.reset_index()\n",
    "df_all = df_all.drop(['index'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "distributed-transmission",
   "metadata": {},
   "outputs": [],
   "source": [
    "status_dict = df_all['label'].unique().tolist()\n",
    "df_all['d_label'] = df_all['label'].apply(lambda x : status_dict.index(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "comfortable-subject",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>content</th>\n",
       "      <th>d_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>体育</td>\n",
       "      <td>马晓旭意外受伤让国奥警惕 无奈大雨格外青睐殷家军记者傅亚雨沈阳报道 来到沈阳，国奥队依然没有...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>体育</td>\n",
       "      <td>商瑞华首战复仇心切 中国玫瑰要用美国方式攻克瑞典多曼来了，瑞典来了，商瑞华首战求3分的信心也...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>体育</td>\n",
       "      <td>冠军球队迎新欢乐派对 黄旭获大奖张军赢下PK赛新浪体育讯12月27日晚，“冠军高尔夫球队迎新...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>体育</td>\n",
       "      <td>辽足签约危机引注册难关 高层威逼利诱合同笑里藏刀新浪体育讯2月24日，辽足爆发了集体拒签风波...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>体育</td>\n",
       "      <td>揭秘谢亚龙被带走：总局电话骗局 复制南杨轨迹体坛周报特约记者张锐北京报道  谢亚龙已经被公安...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  label                                            content  d_label\n",
       "0    体育  马晓旭意外受伤让国奥警惕 无奈大雨格外青睐殷家军记者傅亚雨沈阳报道 来到沈阳，国奥队依然没有...        0\n",
       "1    体育  商瑞华首战复仇心切 中国玫瑰要用美国方式攻克瑞典多曼来了，瑞典来了，商瑞华首战求3分的信心也...        0\n",
       "2    体育  冠军球队迎新欢乐派对 黄旭获大奖张军赢下PK赛新浪体育讯12月27日晚，“冠军高尔夫球队迎新...        0\n",
       "3    体育  辽足签约危机引注册难关 高层威逼利诱合同笑里藏刀新浪体育讯2月24日，辽足爆发了集体拒签风波...        0\n",
       "4    体育  揭秘谢亚龙被带走：总局电话骗局 复制南杨轨迹体坛周报特约记者张锐北京报道  谢亚龙已经被公安...        0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_all.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "precise-april",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'160万小将让人又爱又恨 如此不稳流言纷飞该怪谁新浪体育讯华盛顿奇才86-95不敌印第安纳步行者，贾维尔-麦基拿下7分3个篮板。本场比赛来到步行者的主场，但是麦基没能再次发挥出应有的作用。本场比赛他自己没有盖帽不说，还让希伯特拿下17分8个篮板。希伯特在场上打了31分钟，反倒是麦基自己的上场时间缩水到18分钟。奇才内线基本上是不设防的，他们让对方五人得分上双，尽管今天步行者的三分球命中率一般，19投仅仅5中，但是除了三分球之外他们59投33中。作为奇才的防守中坚，麦基要负起主要责任。在上一场比赛结束之后，代理主教练兰迪-惠特曼对麦基是赞不绝口，说他们之前研究过比赛录像带，而麦基在比赛中完全执行了教练的意图。但是本场比赛结束之后，麦基又从两天前人们口中的防守天才变成了蠢货。比赛中他并不是没有亮点，开场麦基就让奇才首先得分，在安德雷-布莱切投篮不中的情况下，他还扛起进攻大旗，在希伯特面前玩起了篮下脚步，轻松晃过命中。但是他出糗的场景更多。比赛开始之后他尝试扣篮被希伯特犯规，但是回到防守端，绕前防守希伯特被后者双手暴扣示威。此后他在篮下抢到篮板，二次进攻的时候又遭到迈克-邓利维的封盖。第三节开始之后，麦基再次得到二次进攻机会，但是他的补扣却砸框弹飞。此后他终于觅得表演机会，步行者的外线传球被麦基一把抢断，他带球直冲本方禁区，气势无人可挡，最后以一记单臂暴扣结束了一条龙的表演。可惜他的彪悍表演刚刚开始就被掐灭了，回到防守端希伯特马上就还以颜色，他强打麦基造成后者犯规，这是麦基本场比赛的第四次犯规，他不得不下场休息。菲利普-桑德斯刚一回来，麦基就犯了老毛病，这让桑帅有点气不打一处来。本以为麦基可以凭借对阵希伯特的表现成为奇才防守的依靠，哪知道两天内第二次会面他就露馅了。不知道桑帅会不会在心里大骂烂泥扶不上墙。(春水方生)'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_all['content'][2000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "charged-bronze",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = df_all.content.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "heavy-starter",
   "metadata": {},
   "outputs": [],
   "source": [
    "#把句号替换掉？好似不用\n",
    "#sentences = [sentence.replace('。','[SEP] ') + \" [CLS]\" for sentence in sentences]\n",
    "labels = df_all.d_label.values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "alleged-vault",
   "metadata": {},
   "source": [
    "# 分词"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "australian-tribune",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer\n",
    "tokenizer = XLNetTokenizer.from_pretrained('../utils/chinese-xlnet-base', do_lower_case=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "popular-judgment",
   "metadata": {},
   "outputs": [],
   "source": [
    "# xlnet的模型输入需要两个mask，不能依赖torchtext了\n",
    "#格式：Sentence_A + [SEP] + Sentence_B + [SEP] + [CLS]\n",
    "#本中文数据集有多句话组成，由于是新闻，由句号分割"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "rental-benchmark",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenize the first sentence:\n",
      "['▁马', '晓', '旭', '意外', '受伤', '让', '国', '奥', '警', '惕', '▁', '无', '奈', '大雨', '格', '外', '青', '睐', '殷', '家', '军', '记者', '傅', '亚', '雨', '沈阳', '报道', '▁', '来到', '沈阳', ',', '国', '奥', '队', '依然', '没有', '摆脱', '雨水', '的', '困', '扰', '。', '7', '月', '31', '日下午', '6', '点', ',', '国', '奥', '队的', '日常', '训练', '再度', '受到', '大雨', '的', '干扰', ',', '无', '奈', '之下', '队员', '们', '只', '慢', '跑', '了', '25', '分钟', '就', '草', '草', '收', '场', '。', '31', '日上午', '10', '点', ',', '国', '奥', '队', '在', '奥', '体', '中心', '外', '场', '训练', '的时候', ',', '天', '就是', '阴', '沉', '沉', '的', ',', '气象', '预', '报', '显示', '当天', '下午', '沈阳', '就有', '大雨', ',', '但', '幸', '好', '队伍', '上午', '的', '训练', '并没有', '受到', '任何', '干扰', '。', '下午', '6', '点', ',', '当', '球队', '抵达', '训练', '场', '时', ',', '大雨', '已经', '下了', '几个', '小时', ',', '而且', '丝', '毫', '没有', '停', '下来', '的意思', '。', '抱', '着', '试', '一', '试', '的', '态度', ',', '球队', '开始了', '当天', '下午', '的', '例', '行', '训练', ',', '25', '分钟', '过去', '了', ',', '天气', '没有任何', '转', '好的', '迹', '象', ',', '为了', '保护', '球员', '们', ',', '国', '奥', '队', '决定', '中止', '当天', '的', '训练', ',', '全', '队', '立即', '返回', '酒店', '。', '在', '雨', '中', '训练', '对', '足球队', '来说', '并不是', '什么', '稀', '罕', '事', ',', '但在', '奥运会', '即将', '开始', '之前', ',', '全', '队', '变得', '“', '娇', '贵', '”', '了', '。', '在', '沈阳', '最后一', '周', '的', '训练', ',', '国', '奥', '队', '首先', '要', '保证', '现有', '的', '球员', '不再', '出现', '意外', '的', '伤', '病', '情况', '以免', '影响', '正式', '比赛', ',', '因此', '这一', '阶段', '控制', '训练', '受伤', '、', '控制', '感', '冒', '等', '疾病', '的', '出现', '被', '队伍', '放在', '了', '相当', '重要的', '位置', '。', '而', '抵达', '沈阳', '之后', ',', '中', '后卫', '冯', '萧', '霆', '就', '一直', '没有', '训练', ',', '冯', '萧', '霆', '是', '7', '月', '27', '日在', '长春', '患上', '了', '感', '冒', ',', '因此', '也没有', '参加', '29', '日', '跟', '塞尔维亚', '的', '热', '身', '赛', '。', '队伍', '介绍', '说', ',', '冯', '萧', '霆', '并没有', '出现', '发', '烧', '症状', ',', '但', '为了', '安全', '起', '见', ',', '这两', '天', '还是', '让他', '静', '养', '休息', ',', '等', '感', '冒', '彻底', '好', '了', '之后', '再', '恢复', '训练', '。', '由于', '有了', '冯', '萧', '霆', '这个', '例子', ',', '因此', '国', '奥', '队', '对', '雨', '中', '训练', '就', '显得', '特别', '谨', '慎', ',', '主要是', '担心', '球员', '们', '受', '凉', '而', '引发', '感', '冒', ',', '造成', '非', '战斗', '减', '员', '。', '而', '女', '足', '队员', '马', '晓', '旭', '在', '热', '身', '赛中', '受伤', '导致', '无', '缘', '奥运', '的前', '科', ',', '也', '让', '在', '沈阳', '的', '国', '奥', '队', '现在', '格', '外', '警', '惕', ',', '“', '训练', '中', '不断', '嘱', '咐', '队员', '们', '要', '注意', '动作', ',', '我们', '可', '不能', '再', '出', '这样', '的事情', '了', '。', '”', '一位', '工作人员', '表示', '。', '从', '长春', '到', '沈阳', ',', '雨水', '一路', '伴随着', '国', '奥', '队', ',', '“', '也', '邪', '了', ',', '我们', '走到', '哪', '儿', '雨', '就', '下', '到', '哪', '儿', ',', '在', '长春', '几次', '训练', '都被', '大雨', '给', '搅', '和', '了', ',', '没', '想到', '来', '沈阳', '又', '碰', '到', '这种', '事情', '。', '”', '一位', '国', '奥', '球员', '也', '对', '雨水', '的', '“', '青', '睐', '”', '有些', '不', '解', '。']\n"
     ]
    }
   ],
   "source": [
    "tokenized_texts = [tokenizer.tokenize(sent) for sent in sentences]\n",
    "print (\"Tokenize the first sentence:\")\n",
    "print (tokenized_texts[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "appreciated-faith",
   "metadata": {},
   "outputs": [],
   "source": [
    "#设备限制，虽然xlnet支持长段落，但这里还是128稳妥\n",
    "MAX_LEN = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "optical-adaptation",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids = [tokenizer.convert_tokens_to_ids(x) for x in tokenized_texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "surprised-romance",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids = pad_sequences(input_ids, maxlen=MAX_LEN, dtype=\"long\", truncating=\"post\", padding=\"post\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "distinct-helen",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create attention masks\n",
    "attention_masks = []\n",
    "\n",
    "# Create a mask of 1s for each token followed by 0s for padding\n",
    "for seq in input_ids:\n",
    "    seq_mask = [float(i>0) for i in seq]\n",
    "    attention_masks.append(seq_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "moving-criterion",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_inputs, validation_inputs, train_labels, validation_labels = train_test_split(input_ids, labels, \n",
    "                                                            random_state=SEED, test_size=0.1)\n",
    "train_masks, validation_masks, _, _ = train_test_split(attention_masks, input_ids,\n",
    "                                             random_state=SEED, test_size=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "essential-employer",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_inputs = torch.tensor(train_inputs)\n",
    "validation_inputs = torch.tensor(validation_inputs)\n",
    "train_labels = torch.tensor(train_labels)\n",
    "validation_labels = torch.tensor(validation_labels)\n",
    "train_masks = torch.tensor(train_masks)\n",
    "validation_masks = torch.tensor(validation_masks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "illegal-numbers",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 16\n",
    "\n",
    "# iterator of our data with torch DataLoader. \n",
    "\n",
    "train_data = TensorDataset(train_inputs, train_masks, train_labels)\n",
    "train_sampler = RandomSampler(train_data)\n",
    "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
    "\n",
    "validation_data = TensorDataset(validation_inputs, validation_masks, validation_labels)\n",
    "validation_sampler = SequentialSampler(validation_data)\n",
    "validation_dataloader = DataLoader(validation_data, sampler=validation_sampler, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "facial-bottom",
   "metadata": {},
   "source": [
    "# MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "honey-hayes",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at ../utils/chinese-xlnet-base were not used when initializing XLNetForSequenceClassification: ['lm_loss.weight', 'lm_loss.bias']\n",
      "- This IS expected if you are initializing XLNetForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLNetForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of XLNetForSequenceClassification were not initialized from the model checkpoint at ../utils/chinese-xlnet-base and are newly initialized: ['sequence_summary.summary.weight', 'sequence_summary.summary.bias', 'logits_proj.weight', 'logits_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "XLNetForSequenceClassification(\n",
       "  (transformer): XLNetModel(\n",
       "    (word_embedding): Embedding(32000, 768)\n",
       "    (layer): ModuleList(\n",
       "      (0): XLNetLayer(\n",
       "        (rel_attn): XLNetRelativeAttention(\n",
       "          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ff): XLNetFeedForward(\n",
       "          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (layer_1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (layer_2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (1): XLNetLayer(\n",
       "        (rel_attn): XLNetRelativeAttention(\n",
       "          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ff): XLNetFeedForward(\n",
       "          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (layer_1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (layer_2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (2): XLNetLayer(\n",
       "        (rel_attn): XLNetRelativeAttention(\n",
       "          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ff): XLNetFeedForward(\n",
       "          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (layer_1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (layer_2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (3): XLNetLayer(\n",
       "        (rel_attn): XLNetRelativeAttention(\n",
       "          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ff): XLNetFeedForward(\n",
       "          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (layer_1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (layer_2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (4): XLNetLayer(\n",
       "        (rel_attn): XLNetRelativeAttention(\n",
       "          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ff): XLNetFeedForward(\n",
       "          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (layer_1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (layer_2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (5): XLNetLayer(\n",
       "        (rel_attn): XLNetRelativeAttention(\n",
       "          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ff): XLNetFeedForward(\n",
       "          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (layer_1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (layer_2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (6): XLNetLayer(\n",
       "        (rel_attn): XLNetRelativeAttention(\n",
       "          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ff): XLNetFeedForward(\n",
       "          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (layer_1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (layer_2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (7): XLNetLayer(\n",
       "        (rel_attn): XLNetRelativeAttention(\n",
       "          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ff): XLNetFeedForward(\n",
       "          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (layer_1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (layer_2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (8): XLNetLayer(\n",
       "        (rel_attn): XLNetRelativeAttention(\n",
       "          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ff): XLNetFeedForward(\n",
       "          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (layer_1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (layer_2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (9): XLNetLayer(\n",
       "        (rel_attn): XLNetRelativeAttention(\n",
       "          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ff): XLNetFeedForward(\n",
       "          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (layer_1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (layer_2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (10): XLNetLayer(\n",
       "        (rel_attn): XLNetRelativeAttention(\n",
       "          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ff): XLNetFeedForward(\n",
       "          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (layer_1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (layer_2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (11): XLNetLayer(\n",
       "        (rel_attn): XLNetRelativeAttention(\n",
       "          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ff): XLNetFeedForward(\n",
       "          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (layer_1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (layer_2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (sequence_summary): SequenceSummary(\n",
       "    (summary): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (first_dropout): Identity()\n",
       "    (last_dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (logits_proj): Linear(in_features=768, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = XLNetForSequenceClassification.from_pretrained(\"../utils/chinese-xlnet-base\", num_labels=10)\n",
    "model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "nearby-lancaster",
   "metadata": {},
   "outputs": [],
   "source": [
    "param_optimizer = list(model.named_parameters())\n",
    "no_decay = ['bias', 'gamma', 'beta']\n",
    "optimizer_grouped_parameters = [\n",
    "    {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)],\n",
    "     'weight_decay_rate': 0.01},\n",
    "    {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)],\n",
    "     'weight_decay_rate': 0.0}\n",
    "]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "entire-mississippi",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = AdamW(optimizer_grouped_parameters,\n",
    "                     lr=2e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "featured-virginia",
   "metadata": {},
   "outputs": [],
   "source": [
    "def flat_accuracy(preds, labels):\n",
    "    pred_flat = np.argmax(preds, axis=1).flatten()\n",
    "    labels_flat = labels.flatten()\n",
    "    return np.sum(pred_flat == labels_flat) / len(labels_flat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "solved-coffee",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch:   0%|          | 0/4 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.14419138695837638\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch:  25%|██▌       | 1/4 [15:01<45:03, 901.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.9738943488943489\n",
      "Train loss: 0.05280072755698995\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch:  50%|█████     | 2/4 [30:02<30:02, 901.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.9768120393120393\n",
      "Train loss: 0.030269932866221667\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch:  75%|███████▌  | 3/4 [45:03<15:01, 901.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.9768120393120393\n",
      "Train loss: 0.02112672677192899\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 100%|██████████| 4/4 [1:00:04<00:00, 901.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.9761977886977887\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Store our loss and accuracy for plotting\n",
    "train_loss_set = []\n",
    "\n",
    "# Number of training epochs (authors recommend between 2 and 4)\n",
    "epochs = 4\n",
    "\n",
    "# trange is a tqdm wrapper around the normal python range\n",
    "for _ in trange(epochs, desc=\"Epoch\"):\n",
    "  \n",
    "  \n",
    "  # Training\n",
    "  \n",
    "  # Set our model to training mode (as opposed to evaluation mode)\n",
    "    model.train()\n",
    "  \n",
    "  # Tracking variables\n",
    "    tr_loss = 0\n",
    "    nb_tr_examples, nb_tr_steps = 0, 0\n",
    "\n",
    "    # Train the data for one epoch\n",
    "    for step, batch in enumerate(train_dataloader):\n",
    "        # Add batch to GPU\n",
    "        batch = tuple(t.to(device) for t in batch)\n",
    "        # Unpack the inputs from our dataloader\n",
    "        b_input_ids, b_input_mask, b_labels = batch\n",
    "        # Clear out the gradients (by default they accumulate)\n",
    "        optimizer.zero_grad()\n",
    "        # Forward pass\n",
    "        outputs = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask, labels=b_labels)\n",
    "        loss = outputs[0]\n",
    "        logits = outputs[1]\n",
    "        train_loss_set.append(loss.item())    \n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        # Update parameters and take a step using the computed gradient\n",
    "        optimizer.step()\n",
    "\n",
    "\n",
    "        # Update tracking variables\n",
    "        tr_loss += loss.item()\n",
    "        nb_tr_examples += b_input_ids.size(0)\n",
    "        nb_tr_steps += 1\n",
    "\n",
    "    print(\"Train loss: {}\".format(tr_loss/nb_tr_steps))\n",
    "\n",
    "\n",
    "    # Validation\n",
    "\n",
    "    # Put model in evaluation mode to evaluate loss on the validation set\n",
    "    model.eval()\n",
    "\n",
    "    # Tracking variables \n",
    "    eval_loss, eval_accuracy = 0, 0\n",
    "    nb_eval_steps, nb_eval_examples = 0, 0\n",
    "\n",
    "    # Evaluate data for one epoch\n",
    "    for batch in validation_dataloader:\n",
    "        # Add batch to GPU\n",
    "        batch = tuple(t.to(device) for t in batch)\n",
    "        # Unpack the inputs from our dataloader\n",
    "        b_input_ids, b_input_mask, b_labels = batch\n",
    "        # Telling the model not to compute or store gradients, saving memory and speeding up validation\n",
    "        with torch.no_grad():\n",
    "            # Forward pass, calculate logit predictions\n",
    "            output = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask)\n",
    "            logits = output[0]\n",
    "\n",
    "        # Move logits and labels to CPU\n",
    "        logits = logits.detach().cpu().numpy()\n",
    "        label_ids = b_labels.to('cpu').numpy()\n",
    "\n",
    "        tmp_eval_accuracy = flat_accuracy(logits, label_ids)\n",
    "\n",
    "        eval_accuracy += tmp_eval_accuracy\n",
    "        nb_eval_steps += 1\n",
    "\n",
    "    print(\"Validation Accuracy: {}\".format(eval_accuracy/nb_eval_steps))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "answering-contest",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,8))\n",
    "plt.title(\"Training loss\")\n",
    "plt.xlabel(\"Batch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.plot(train_loss_set)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
